[
  {
    "objectID": "feed.html",
    "href": "feed.html",
    "title": "Slides",
    "section": "",
    "text": "Difference in Differences\n\n\nECON526\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nECON 526: Quantitative Economics with Data Science Applications\n\n\n\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nMatching\n\n\nECON526\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 526: Quantitative Economics with Data Science Applications",
    "section": "",
    "text": "Slides\n\nMatching slides, notebook\n\nReading: chapters 10-12 of Facure (2022) and chapter 14 of Huntington-Klein (2021)\n\nIntroduction to difference in differences, notebook\n\nReading: chapter 13 of Facure (2022)\n\nFixed Effects, notebook\n\nReading: chapter 14 of Facure (2022)\n\nAdvanced difference in differences, notebook\n\nReading: chapter 24 of Facure (2022), Roth et al. (2023), Chaisemartin and D’Haultfœuille (2022)\n\n\n\n\n\n\n\nReferences\n\nChaisemartin, Clément de, and Xavier D’Haultfœuille. 2022. “Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey.” The Econometrics Journal 26 (3): C1–30. https://doi.org/10.1093/ectj/utac017.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nRoth, Jonathan, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–44. https://doi.org/https://doi.org/10.1016/j.jeconom.2023.03.008."
  },
  {
    "objectID": "matching.html#setting",
    "href": "matching.html#setting",
    "title": "Matching",
    "section": "Setting",
    "text": "Setting\n\nPotential outcomes \\((Y_0, Y_1)\\)\nTreatment \\(T\\)\nObserve \\(Y = Y_0(1-T) + T Y_1\\)\nCovariates \\(X\\)\nAssume conditional independence \\((Y_0,Y_1) \\perp T | X\\)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]"
  },
  {
    "objectID": "matching.html#why-not-regression",
    "href": "matching.html#why-not-regression",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\nAverage treatment effect \\[\nATE = \\int \\Er[Y|T=1,X=x] - \\Er[Y|T=0,X=x] dP(x)\n\\]\nRegression gives the best linear approximation to \\(\\Er[Y|T,X]\\), so why not just estimate linear regression \\[\nY_i = \\hat{\\alpha} T_i + X_i'\\hat{\\beta} + \\hat{\\epsilon}_i\n\\] and, and then use \\(\\hat{\\alpha}\\) as an estimate of the ATE?"
  },
  {
    "objectID": "matching.html#why-not-regression-1",
    "href": "matching.html#why-not-regression-1",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\nPartial out (Frish-Waugh-Lovell theorem) \\[\n\\begin{align*}\n\\hat{\\alpha} = & \\frac{\\frac{1}{n} \\sum_{i=1}^n Y_i (T_i - X_i'(X'X)^{-1}X'T)}\n  {\\frac{1}{n} \\sum_{i=1}^n (T_i - X_i'(X'X)^{-1}X'T)^2} \\\\\n  \\inprob & \\Er\\left[Y_i \\underbrace{\\frac{T_i - X_i'\\pi}{\\Er[(T_i - X_i'\\pi)^2]}}_{\\equiv \\omega(T_i,X_i)}\\right] \\\\\n  = & \\Er\\left[Y_{0,i} \\omega(T_i,X_i)\\right] + \\Er\\left[(Y_{1,i}-Y_{0,i}) \\omega(T_i,X_i)T_i\\right]\n\\end{align*}\n\\] where \\(\\pi = \\argmin_{\\tilde{\\pi}} \\Er[(T_i - X_i'\\tilde{\\pi})^2]\\)\nNote: \\(\\Er[\\omega(T,X)] = 0\\), \\(\\Er[T\\omega(T,X)] = 1\\)"
  },
  {
    "objectID": "matching.html#why-not-regression-2",
    "href": "matching.html#why-not-regression-2",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\\(\\plim \\hat{\\alpha} = \\Er\\left[Y_{0,i} \\omega(T_i,X_i)\\right] + \\Er\\left[(Y_{1,i}-Y_{0,i}) \\omega(T_i,X_i)T_i\\right]\\)\nWhat can be in the range of \\(\\omega(T,X) = \\frac{T - X'\\pi}{\\Er[(T_i - X_i'\\pi)^2]}\\)?"
  },
  {
    "objectID": "matching.html#why-not-regression-3",
    "href": "matching.html#why-not-regression-3",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\nimports\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nstyle.use(\"fivethirtyeight\")\n\n\n\nnp.random.seed(1234)\n\ndef simulate(n, pi=np.array([0,1])):\n    X = np.random.randn(n, len(pi))\n    X[:,0] = 1\n    T = 1*((X @ pi + np.random.randn(n))&gt;0)\n    y0 = np.random.randn(n)\n    y1 = np.exp(3*(X[:,1]-2)) + np.random.randn(n)\n    y = T*y1 + (1-T)*y0\n    return(X,T,y,y0,y1)\n\nX,T,y,y0,y1 = simulate(1000)\n\npihat = np.linalg.solve(X.T @ X, X.T @ T)\nw = T - X @ pihat\nw = w/np.mean(w**2);"
  },
  {
    "objectID": "matching.html#why-not-regression-4",
    "href": "matching.html#why-not-regression-4",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\nTX = np.hstack((T.reshape(len(T),1),X))\nabhat = np.linalg.solve(TX.T @ TX, TX.T @ y)\nahat = abhat[0]\ndisplay(ahat)\n\n-0.06414016921951447\n\n\n\nnp.mean(y1-y0)\n\n0.22383059765273303\n\n\n\nWeights, \\(\\omega(T,X)\\), are not all positive, so the regression estimate can be negative even if \\(\\Er[Y_1 | X] - \\Er[Y_0|X]\\) is positive everywhere"
  },
  {
    "objectID": "matching.html#why-not-regression-5",
    "href": "matching.html#why-not-regression-5",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\nplot\nimport matplotlib.cm as cm\nfig, axes = plt.subplots(2, 1, figsize=(6, 6))\n\n# Create a scatter plot for the first panel (left)\naxes[0].scatter(X[:,1], w, c=T, cmap=cm.Dark2)\naxes[0].set_xlabel(\"X\")\naxes[0].set_ylabel(\"ωT\")\naxes[0].set_title(\"Weights\")\n\naxes[1].scatter(X[:,1], y1-y0, label=\"TE\")\naxes[1].set_xlabel(\"X\")\naxes[1].set_ylabel(\"Y₁ - Y₀\")\naxes[1].set_title(\"Treatment effects\")\n\n\n# Display the plot\nplt.tight_layout()  # Ensure proper layout spacing\nplt.show()"
  },
  {
    "objectID": "matching.html#matching-1",
    "href": "matching.html#matching-1",
    "title": "Matching",
    "section": "Matching",
    "text": "Matching\n\nIf not regression, then what? \\[\nATE = \\int \\Er[Y|T=1,X=x] - \\Er[Y|T=0,X=x] dP(x)\n\\]"
  },
  {
    "objectID": "matching.html#propensity-score",
    "href": "matching.html#propensity-score",
    "title": "Matching",
    "section": "Propensity Score",
    "text": "Propensity Score\n\nLet \\(e(X) = P(T=1|X=X)\\)\nNote: \\[\n\\begin{align*}\n\\Er[Y|X,T=1] - \\Er[Y|X,T=0] = & E\\left[\\frac{Y T}{e(X)}|X \\right] - E\\left[\\frac{Y(1-T)}{1-e(X)}|X \\right] \\\\\n= & E\\left[ Y \\frac{T - e(X)}{e(X)(1-e(X))} | X \\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "matching.html#inverse-probability-weighting",
    "href": "matching.html#inverse-probability-weighting",
    "title": "ECON526: Quantitative Economics with Data Science Applications",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\n\nEstimator \\[\n\\widehat{ATE}^{IPW} = \\frac{1}{n} \\sum_{i=1}^n \\frac{Y_iT_i}{\\hat{e}(X_i)} - \\frac{Y_i(1-T_i)}{1-\\hat{e}(X_i)}\n\\] where \\(\\hat{e}(X)\\) is some flexible estimator for \\(P(T=1|X)\\)\nDownside:\n\nDifficult statistical properties — choice of tuning parameters, strong assumptions needed"
  },
  {
    "objectID": "matching.html#doubly-robust-estimator",
    "href": "matching.html#doubly-robust-estimator",
    "title": "Matching",
    "section": "Doubly Robust Estimator",
    "text": "Doubly Robust Estimator\n\nEstimator \\[\n\\begin{align*}\n\\widehat{ATE}^{DR} = & \\frac{1}{n} \\sum_{i=1}^n \\hat{E}[Y|T=1,X=X_i] - \\hat{E}[Y|T=0,X=X_i] + \\\\\n& + \\frac{1}{n} \\sum_{i=1}^n  \\frac{T_i(Y_i - \\hat{E}[Y|T=1,X=X_i])}{\\hat{e}(X_i)} - \\\\\n& - \\frac{(1-T_i)(Y_i - \\hat{E}[Y|T=0,X=X_i])} {1-\\hat{e}(X_i)}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "matching.html#software",
    "href": "matching.html#software",
    "title": "Matching",
    "section": "Software",
    "text": "Software\n\nAdvice: use the doubly robust estimator with nonparametric estimates for \\(\\hat{E}[Y|T,X]\\) and \\(\\hat{e}(X)\\)\nRecommended package:\n\neconml has the correct estimator and examples of using it with nonparametric estimates\n\nfocuses on conditional instead of unconditional average treatment effects, but can be used for both"
  },
  {
    "objectID": "matching.html#example",
    "href": "matching.html#example",
    "title": "ECON526: Quantitative Economics with Data Science Applications",
    "section": "Example",
    "text": "Example\n\nfrom econml.dr import DRLearner, LinearDRLearner, SparseLinearDRLearner\nfrom econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures#\n\nest = LinearDRLearner(featurizer=PolynomialFeatures(degree=20, include_bias=False),\n                model_regression=LassoCV(),\n                model_propensity=LogisticRegressionCV(),\n                #model_final=StatsModelsLinearRegression(),\n                cv=10)\nest.fit(y, T, X=None, W=X)\n\n&lt;econml.dr._drlearner.LinearDRLearner at 0x7face555b190&gt;\n\n\n\npoint = est.const_marginal_effect(None)\nlb, ub = est.const_marginal_effect_interval(None, alpha=0.05)\ndisplay(lb,point,ub)\n\narray([[-0.03922896]])\n\n\narray([[0.12089912]])\n\n\narray([[0.28102719]])"
  },
  {
    "objectID": "matching.html#references",
    "href": "matching.html#references",
    "title": "Matching",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nAbadie, Alberto, and Guido W. Imbens. 2008. “On the Failure of the Bootstrap for Matching Estimators.” Econometrica 76 (6): 1537–57. https://doi.org/https://doi.org/10.3982/ECTA6474.\n\n\nAthey, Susan, and Stefan Wager. 2019. “Estimating Treatment Effects with Causal Forests: An Application.”\n\n\nBorusyak, Kirill, and Xavier Jaravel. 2018. “Revisiting Event Study Designs.” https://scholar.harvard.edu/files/borusyak/files/borusyak_jaravel_event_studies.pdf.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nYeager, David S., Paul Hanselman, Gregory M. Walton, Jared S. Murray, Robert Crosnoe, Chandra Muller, Elizabeth Tipton, et al. 2019. “A National Experiment Reveals Where a Growth Mindset Improves Achievement.” Nature 573 (7774): 364–69. https://doi.org/10.1038/s41586-019-1466-y."
  },
  {
    "objectID": "matching.html#why-not-regression-6",
    "href": "matching.html#why-not-regression-6",
    "title": "ECON526: Quantitative Economics with Data Science Applications",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\nplot\nfig, axes = plt.subplots(2, 1, figsize=(8, 6))\n\n# Create a scatter plot for the first panel (left)\naxes[0].scatter(X[:,1], w*T)\naxes[0].set_xlabel(\"X\")\naxes[0].set_ylabel(\"ωT\")\naxes[0].set_title(\"Weights\")\n\naxes[1].scatter(X[:,1], y1-y0, label=\"TE\")\naxes[1].set_xlabel(\"X\")\naxes[1].set_ylabel(\"Y₁ - Y₀\")\naxes[1].set_title(\"Treatment effects\")\n\n\n# Display the plot\nplt.tight_layout()  # Ensure proper layout spacing\nplt.show()"
  },
  {
    "objectID": "matching.html#plug-in-estimator",
    "href": "matching.html#plug-in-estimator",
    "title": "Matching",
    "section": "Plug-in estimator",
    "text": "Plug-in estimator\n\nPlug in estimator: \\[\n\\widehat{ATE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{E}[Y|T=1,X=X_i] - \\hat{E}[Y|T=0,X=X_i] \\right)\n\\] where \\(\\hat{E}[Y|T,X]\\) is some flexible estimator for \\(\\Er[Y|T,X]\\)\n\nif \\(X\\) is discrete, \\(\\hat{E}\\) can be conditional averages or equivalently, “saturated” regression\nif \\(X\\) continuous, \\(\\hat{E}\\) can be some nonparametric regression estimator\nOriginal approaches to this problem used nearest neighbor matching to estimate \\(\\hat{E}[Y|T,X]\\)\n\nDownside:\n\nDifficult statistical properties — choice of tuning parameters, strong assumptions needed, failure of bootstrap for nearest neighbors Abadie and Imbens (2008)"
  },
  {
    "objectID": "matching.html#doubly-robust-estimator-1",
    "href": "matching.html#doubly-robust-estimator-1",
    "title": "Matching",
    "section": "Doubly Robust Estimator",
    "text": "Doubly Robust Estimator\n\nDoubly robust in that:\n\nConsistent as long as either \\(\\hat{e}(X) \\inprob e(X)\\) or \\(\\hat{E}[Y|T,X] \\inprob \\Er[Y|T,X]\\)\nInsensitive to small changes in \\(\\hat{e}(X)\\) or \\(\\hat{E}[Y|T,X]\\)\n\nAllows: nicer statistical properties\n\nWeaker assumptions needed\nAsymptotic distribution is the same as if \\(e(X)\\) and \\(\\Er[Y|T,X]\\) were known"
  },
  {
    "objectID": "matching.html#example-in-simulation",
    "href": "matching.html#example-in-simulation",
    "title": "ECON526: Quantitative Economics with Data Science Applications",
    "section": "Example: in simulation",
    "text": "Example: in simulation\n\nInfeasible estimator: average of \\(Y_1 - Y_0\\)\n\n\nse = np.sqrt(np.var(y1-y0)/len(y1))\nate = np.mean(y1-y0)\ndisplay(ate-1.96*se, ate, ate+1.96*se)\n\n0.10010844404085255\n\n\n0.22383059765273303\n\n\n0.3475527512646135\n\n\n\nfrom econml.dr import DRLearner, LinearDRLearner, SparseLinearDRLearner\nfrom econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures#\n\nest = LinearDRLearner(featurizer=PolynomialFeatures(degree=20, include_bias=False),\n                model_regression=LassoCV(),\n                model_propensity=LogisticRegressionCV(),\n                #model_final=StatsModelsLinearRegression(),\n                cv=10)\nest.fit(y, T, X=None, W=X)\n\n&lt;econml.dr._drlearner.LinearDRLearner at 0x7fae4bf8d250&gt;\n\n\n\npoint = est.const_marginal_effect(None)\nlb, ub = est.const_marginal_effect_interval(None, alpha=0.05)\ndisplay(lb,point,ub)\n\narray([[-0.03922896]])\n\n\narray([[0.12089912]])\n\n\narray([[0.28102719]])"
  },
  {
    "objectID": "matching.html#sources-and-further-reading",
    "href": "matching.html#sources-and-further-reading",
    "title": "Matching",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nUseful additional reading is chapters 10-12 of Facure (2022) and chapter 14 of Huntington-Klein (2021).1\nThe representation of the estimate from a linear model as a weighted average is based on Borusyak and Jaravel (2018)\nThe growth mindset example is take from Facure (2022)\n\nThese slides do not mention the importance of overlap/balance, but hopefully I emphasized it during lecture. Overlap is very important in practice. The reading, especially Huntington-Klein (2021), cover it pretty well."
  },
  {
    "objectID": "matching.html#example-simulation",
    "href": "matching.html#example-simulation",
    "title": "Matching",
    "section": "Example: simulation",
    "text": "Example: simulation\n\nInfeasible estimator: average of \\(Y_1 - Y_0\\)\n\n\nse = np.sqrt(np.var(y1-y0)/len(y1))\nate = np.mean(y1-y0)\ndisplay(ate-1.96*se, ate, ate+1.96*se)\n\n0.10010844404085255\n\n\n0.22383059765273303\n\n\n0.3475527512646135"
  },
  {
    "objectID": "matching.html#example-simulation-1",
    "href": "matching.html#example-simulation-1",
    "title": "Matching",
    "section": "Example: simulation",
    "text": "Example: simulation\n\nfrom econml.dr import DRLearner, LinearDRLearner, SparseLinearDRLearner\nfrom econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures#\n\nest = LinearDRLearner(featurizer=PolynomialFeatures(degree=20, include_bias=False),\n                model_regression=LassoCV(),\n                model_propensity=LogisticRegressionCV(),\n                #model_final=StatsModelsLinearRegression(),\n                cv=10)\nest.fit(y, T, X=None, W=X)\n\n&lt;econml.dr._drlearner.LinearDRLearner at 0x7f76dda00090&gt;\n\n\n\npoint = est.const_marginal_effect(None)\nlb, ub = est.const_marginal_effect_interval(None, alpha=0.05)\nprint(lb,point,ub)\n\n[[-0.03922896]] [[0.12089912]] [[0.28102719]]"
  },
  {
    "objectID": "matching.html#national-study-of-learning-mindsets",
    "href": "matching.html#national-study-of-learning-mindsets",
    "title": "Matching",
    "section": "National Study of Learning Mindsets",
    "text": "National Study of Learning Mindsets\n\nOriginal study by Yeager et al. (2019)\nSynthetic data created by Athey and Wager (2019), downloaded from Facure (2022)"
  },
  {
    "objectID": "matching.html#data",
    "href": "matching.html#data",
    "title": "Matching",
    "section": "Data",
    "text": "Data\n\n\nimports\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nfrom causalinference import CausalModel\nstyle.use(\"fivethirtyeight\")\npd.set_option(\"display.max_columns\", 20)\ndatadir=\"./data\"\n\n\n\ndata = pd.read_csv(datadir+\"/learning_mindset.csv\")\ndata.sample(5, random_state=431)\n\n\n\n\n\n\n\n\nschoolid\nintervention\nachievement_score\nsuccess_expect\nethnicity\ngender\nfrst_in_family\nschool_urbanicity\nschool_mindset\nschool_achievement\nschool_ethnic_minority\nschool_poverty\nschool_size\n\n\n\n\n9366\n9\n0\n1.137192\n6\n1\n1\n1\n4\n1.324323\n-1.311438\n1.930281\n0.281143\n0.362031\n\n\n7810\n27\n0\n-0.554268\n5\n2\n1\n1\n1\n0.240267\n-0.785287\n0.611807\n0.612568\n-0.116284\n\n\n7532\n29\n0\n-0.462576\n6\n1\n1\n1\n1\n-0.373087\n0.113096\n-0.833417\n-1.924778\n-1.147314\n\n\n10381\n1\n0\n-0.402644\n5\n2\n2\n1\n3\n1.185986\n-1.129889\n1.009875\n1.005063\n-1.174702\n\n\n1244\n57\n1\n1.528680\n6\n4\n1\n1\n2\n0.097162\n-0.292353\n-1.030865\n-0.813799\n0.184716"
  },
  {
    "objectID": "matching.html#evidence-of-confounding",
    "href": "matching.html#evidence-of-confounding",
    "title": "Matching",
    "section": "Evidence of Confounding",
    "text": "Evidence of Confounding\n\n\nCode\ndef std_error(x):\n    return np.std(x, ddof=1) / np.sqrt(len(x))\n\ngrouped = data.groupby('success_expect')['intervention'].agg(['mean', std_error])\ngrouped = grouped.reset_index()\n\nfig, ax = plt.subplots()\nplt.errorbar(grouped['success_expect'],grouped['mean'],yerr=1.96*grouped['std_error'],fmt=\"o\")\nax.set_xlabel('student expectation of success')\nax.set_ylabel('P(treatment)')\nplt.show()"
  },
  {
    "objectID": "matching.html#unadjusted-estimate-of-ate",
    "href": "matching.html#unadjusted-estimate-of-ate",
    "title": "Matching",
    "section": "Unadjusted estimate of ATE",
    "text": "Unadjusted estimate of ATE\n\nprint(smf.ols(\"achievement_score ~ intervention\", data=data).fit().summary().tables[1])\n\n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept       -0.1538      0.012    -13.201      0.000      -0.177      -0.131\nintervention     0.4723      0.020     23.133      0.000       0.432       0.512\n================================================================================\n\n\n\nprint(smf.ols(\"achievement_score ~ intervention\", data=data).fit(\n    cov_type=\"cluster\", cov_kwds={'groups': data['schoolid']}).summary().tables[1])\n\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept       -0.1538      0.036     -4.275      0.000      -0.224      -0.083\nintervention     0.4723      0.025     19.184      0.000       0.424       0.521\n================================================================================"
  },
  {
    "objectID": "matching.html#regression-estimate-of-ate",
    "href": "matching.html#regression-estimate-of-ate",
    "title": "Matching",
    "section": "Regression estimate of ATE",
    "text": "Regression estimate of ATE\n\nols = smf.ols(\"achievement_score ~ intervention + success_expect + ethnicity + gender + frst_in_family + school_urbanicity + school_mindset + school_achievement + school_ethnic_minority + school_poverty + school_size\",data=data).fit()\nprint(ols.summary().tables[1])\n\n==========================================================================================\n                             coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                 -1.7786      0.056    -31.880      0.000      -1.888      -1.669\nintervention               0.3964      0.018     22.192      0.000       0.361       0.431\nsuccess_expect             0.3746      0.008     49.514      0.000       0.360       0.389\nethnicity                  0.0043      0.002      2.049      0.040       0.000       0.008\ngender                    -0.2684      0.017    -16.060      0.000      -0.301      -0.236\nfrst_in_family            -0.1310      0.018     -7.248      0.000      -0.166      -0.096\nschool_urbanicity          0.0573      0.007      8.240      0.000       0.044       0.071\nschool_mindset            -0.1484      0.011    -13.083      0.000      -0.171      -0.126\nschool_achievement        -0.0253      0.013     -1.902      0.057      -0.051       0.001\nschool_ethnic_minority     0.1197      0.011     11.178      0.000       0.099       0.141\nschool_poverty            -0.0154      0.011     -1.466      0.143      -0.036       0.005\nschool_size               -0.0467      0.011     -4.326      0.000      -0.068      -0.026\n=========================================================================================="
  },
  {
    "objectID": "matching.html#regression-estimate-of-ate-weights",
    "href": "matching.html#regression-estimate-of-ate-weights",
    "title": "Matching",
    "section": "Regression estimate of ATE: weights",
    "text": "Regression estimate of ATE: weights\n\nlpm = smf.ols(\"intervention ~ success_expect + ethnicity + gender + frst_in_family + school_urbanicity + school_mindset + school_achievement + school_ethnic_minority + school_poverty + school_size\",data=data).fit(cov_type=\"cluster\", cov_kwds={'groups': data['schoolid']})\nw = lpm.resid / np.var(lpm.resid)\nprint(np.mean(data.achievement_score*w))\n\n0.39640236033389553"
  },
  {
    "objectID": "matching.html#propensity-score-matching",
    "href": "matching.html#propensity-score-matching",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\ncateg = [\"ethnicity\", \"gender\", \"school_urbanicity\",\"success_expect\"]\ncont = [\"school_mindset\", \"school_achievement\", \"school_ethnic_minority\", \"school_poverty\", \"school_size\"]\n\ndata_with_categ = pd.concat([\n    data.drop(columns=categ), # dataset without the categorical features\n    pd.get_dummies(data[categ], columns=categ, drop_first=False)# categorical features converted to dummies\n], axis=1)\n\nprint(data_with_categ.shape)\nT = 'intervention'\nY = 'achievement_score'\nX = data_with_categ.columns.drop(['schoolid', T, Y])\n\n(10391, 38)"
  },
  {
    "objectID": "matching.html#inverse-propensity-weighting",
    "href": "matching.html#inverse-propensity-weighting",
    "title": "Matching",
    "section": "Inverse propensity weighting",
    "text": "Inverse propensity weighting\n\nEstimator \\[\n\\widehat{ATE}^{IPW} = \\frac{1}{n} \\sum_{i=1}^n \\frac{Y_iT_i}{\\hat{e}(X_i)} - \\frac{Y_i(1-T_i)}{1-\\hat{e}(X_i)}\n\\] where \\(\\hat{e}(X)\\) is some flexible estimator for \\(P(T=1|X)\\)\nDownside:\n\nDifficult statistical properties — choice of tuning parameters, strong assumptions needed"
  },
  {
    "objectID": "matching.html#doubly-robust",
    "href": "matching.html#doubly-robust",
    "title": "Matching",
    "section": "Doubly Robust",
    "text": "Doubly Robust\n\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef robustate(T,Y,X,psmodel=LogisticRegressionCV(),ymodel=LassoCV(), cluster=None):\n    pfit = psmodel.fit(X,T)\n    ps = pfit.predict_proba(X)[:,1]\n    ey1fit = ymodel.fit(X[T==1],Y[T==1])\n    ey0fit = sklearn.base.clone(ymodel).fit(X[T==0],Y[T==0])\n    ey1 = ey1fit.predict(X)\n    ey0 = ey0fit.predict(X)\n    ate_terms = ey1 - ey0 + T*(Y- ey1)/ps - (1-T)*(Y-ey0)/(1-ps)\n    ate = np.mean(ate_terms)\n    # check if cluster is None\n    if cluster is None :\n        ate_se = np.sqrt(np.var(ate_terms)/len(ate_terms))\n    else :\n        creg=smf.ols(\"y ~ 1\", pd.DataFrame({\"y\" : ate_terms})).fit(cov_type=\"cluster\", cov_kwds={'groups': cluster})\n        ate_se = np.sqrt(creg.cov_params().iloc[0,0])\n\n    return(ate, ate_se, ps, ey1,ey0)\n\nate,se,ps,ey1,ey0 = robustate(data_with_categ[T],data_with_categ[Y],data_with_categ[X],cluster=data_with_categ['schoolid'])\nprint(ate-1.96*se, ate, ate+1.96*se)\n\n0.33214226035254746 0.3836157943394324 0.4350893283263173\n\n\n1\nWe have glossed over some details needed for doubly robust estimation to have nice statistical properties. Those details matter and are not implemented correctly above. It is better to use the econml instead."
  },
  {
    "objectID": "matching.html#doubly-robust-1",
    "href": "matching.html#doubly-robust-1",
    "title": "Matching",
    "section": "Doubly Robust",
    "text": "Doubly Robust\n\nbetter to use the econml package\n\n\nfrom econml.dr import DRLearner, LinearDRLearner, SparseLinearDRLearner\nfrom econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\n\nest = LinearDRLearner(#featurizer=PolynomialFeatures(degree=2, include_bias=False),\n                model_regression=LassoCV(),\n                model_propensity=LogisticRegressionCV(),\n                cv=5)\n\nest.fit(data_with_categ[Y], data_with_categ[T], X=None, W=data_with_categ[X])\npoint = est.const_marginal_effect(None)\nlb, ub = est.const_marginal_effect_interval(None, alpha=0.05)\nprint(lb,point,ub)\n\n[[0.35515763]] [[0.38856831]] [[0.42197899]]"
  },
  {
    "objectID": "matching.html#propensity-score-1",
    "href": "matching.html#propensity-score-1",
    "title": "Matching",
    "section": "Propensity Score",
    "text": "Propensity Score\n\nso \\[\nATE = \\Er\\left[ \\frac{Y T}{e(X)} -  \\frac{Y(1-T)}{1-e(X)}\\right] = \\Er\\left[ Y \\frac{T - e(X)}{e(X)(1-e(X))} \\right]\n\\]"
  },
  {
    "objectID": "matching.html#software-1",
    "href": "matching.html#software-1",
    "title": "Matching",
    "section": "Software",
    "text": "Software\n\nOther packages:\n\ncausalinference has a double robust estimator, but it estimates \\(\\hat{E}[Y|T,X]\\) via linear regression and \\(\\hat{e}(X)\\) via logit (maybe probit, not sure)\n\ncan make nonparametric by adding e.g. powers of \\(x\\) to \\(X\\), but need to manage manually\n\nzEpid is similiar to causalinference, but has a formula interface, so slightly easier to make model more flexible"
  },
  {
    "objectID": "matching.html#unadjusted-estimate-of-ate-1",
    "href": "matching.html#unadjusted-estimate-of-ate-1",
    "title": "Matching",
    "section": "Unadjusted estimate of ATE",
    "text": "Unadjusted estimate of ATE\n\n\nCode\nfig,ax=plt.subplots()\nplt.hist(data.query(\"intervention==0\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C2\")\nplt.hist(data.query(\"intervention==1\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C3\")\nplt.vlines(-0.1538, 0, 300, label=\"Untreated\", color=\"C2\")\nplt.vlines(-0.1538+0.4723, 0, 300, label=\"Treated\", color=\"C3\")\nax.set_xlabel(\"Achievement Score\")\nax.set_ylabel(\"N\")\nplt.legend()\nplt.show();"
  },
  {
    "objectID": "matching.html#regression-estimate-of-ate-weights-1",
    "href": "matching.html#regression-estimate-of-ate-weights-1",
    "title": "Matching",
    "section": "Regression estimate of ATE: weights",
    "text": "Regression estimate of ATE: weights\n\n\nCode\nfig,ax=plt.subplots()\nplt.hist(w[data.intervention==0], bins=20, alpha=0.3, color=\"C2\", label=\"Untreated\")\nplt.hist(w[data.intervention==1], bins=20, alpha=0.3, color=\"C3\", label=\"Treated\")\nax.set_xlabel(\"w\")\nax.set_ylabel(\"N\")\nplt.legend()\nplt.show();"
  },
  {
    "objectID": "matching.html#propensity-score-matching-1",
    "href": "matching.html#propensity-score-matching-1",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsRegressor\nimport sklearn\n\ndef propensitymatching(T,Y,X,psmodel=LogisticRegressionCV(),neighbormodel=KNeighborsRegressor(n_neighbors=1,algorithm='auto',weights='uniform')):\n    pfit = psmodel.fit(X,T)\n    ps = pfit.predict_proba(X)[:,1]\n    ey1 = neighbormodel.fit(ps[T==1].reshape(-1,1),Y[T==1])\n    ey0 = sklearn.base.clone(neighbormodel).fit(ps[T==0].reshape(-1,1),Y[T==0])\n    tex = ey1.predict(ps.reshape(-1,1)) - ey0.predict(ps.reshape(-1,1))\n    ate = np.mean(tex)\n    return(ate, tex,ps)\n\nate,tex,ps=propensitymatching(data_with_categ[T],data_with_categ[Y],data_with_categ[X])\nprint(ate)\n\n0.3423570909254285"
  },
  {
    "objectID": "matching.html#propensity-score-matching-2",
    "href": "matching.html#propensity-score-matching-2",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\n\nCode\nfig, ax = plt.subplots(2,1)\ntreat = data.intervention\nax[0].scatter(ps[treat==0],tex[treat==0],color=\"C2\")\nax[0].scatter(ps[treat==1],tex[treat==1],color=\"C3\")\nax[1].hist(ps[treat==0],bins=20,color=\"C2\",label=\"Untreated\")\nax[1].hist(ps[treat==1],bins=20,color=\"C3\",label=\"Treated\")\nax[1].set_xlabel(\"P(Treatment)\")\nax[1].set_ylabel(\"N\")\nax[0].set_ylabel(\"E[Y|T=1,P(X)] - E[Y|T=0,P(X)]\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "matching.html#inverse-propensity-weighting-1",
    "href": "matching.html#inverse-propensity-weighting-1",
    "title": "Matching",
    "section": "Inverse Propensity Weighting",
    "text": "Inverse Propensity Weighting\n\ndef ipw(T,Y,X,psmodel=LogisticRegressionCV()):\n    pfit = psmodel.fit(X,T)\n    ps = pfit.predict_proba(X)[:,1]\n    ate=np.mean(Y*(T - ps)/(ps*(1-ps)))\n    return(ate,ps)\n\nate,ps = ipw(data_with_categ[T],data_with_categ[Y],data_with_categ[X])\nprint(ate)\n\n0.46498505452715805"
  },
  {
    "objectID": "did.html#introduction-1",
    "href": "did.html#introduction-1",
    "title": "Difference in Differences",
    "section": "Introduction",
    "text": "Introduction\n\nHave some policy applied to some observations but not others, and observe outcome before and after policy\nIdea: compare outcome before and after policy in treated and untreated group\nChange in outcome in treated group reflects both effect of policy and time trend, change in untreated group captures time trend"
  },
  {
    "objectID": "did.html#example-impact-of-billboards",
    "href": "did.html#example-impact-of-billboards",
    "title": "Difference in Differences",
    "section": "Example: Impact of Billboards",
    "text": "Example: Impact of Billboards\n\nFrom Facure (2022) chapter 13\nBank placed billboards advertising savings accounts in Porto Alegre in July\nData on deposits in May and July in Porto Alegre and Florianopolis"
  },
  {
    "objectID": "did.html#example-impact-of-billboards-1",
    "href": "did.html#example-impact-of-billboards-1",
    "title": "Difference in Differences",
    "section": "Example: Impact of Billboards",
    "text": "Example: Impact of Billboards\n\n\nCode\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\ndatadir=\"./data\"\n\n\n\ndata = pd.read_csv(datadir + \"/billboard_impact.csv\")\ndata.head()\n\n\n\n\n\n\n\n\ndeposits\npoa\njul\n\n\n\n\n0\n42\n1\n0\n\n\n1\n0\n1\n0\n\n\n2\n52\n1\n0\n\n\n3\n119\n1\n0\n\n\n4\n21\n1\n0"
  },
  {
    "objectID": "did.html#means-and-differences",
    "href": "did.html#means-and-differences",
    "title": "Difference in Differences",
    "section": "Means and Differences",
    "text": "Means and Differences\n\ntbl = data.groupby(['jul','poa']).mean().unstack()\ntbl\n\n\n\n\n\n\n\n\ndeposits\n\n\npoa\n0\n1\n\n\njul\n\n\n\n\n\n\n0\n171.642308\n46.01600\n\n\n1\n206.165500\n87.06375\n\n\n\n\n\n\n\n\ntbl.diff(axis=0).iloc[1,:]\n\n          poa\ndeposits  0      34.523192\n          1      41.047750\nName: 1, dtype: float64\n\n\n\ntbl.diff(axis=1).iloc[:,1]\n\njul\n0   -125.626308\n1   -119.101750\nName: (deposits, 1), dtype: float64\n\n\n\ntbl.diff(axis=0).diff(axis=1).iloc[1,1]\n\n6.524557692307688"
  },
  {
    "objectID": "did.html#setup",
    "href": "did.html#setup",
    "title": "Difference in Differences",
    "section": "Setup",
    "text": "Setup\n\nTwo periods, binary treatment in second period\nPotential outcomes \\(\\{y_{it}(0),y_{it}(1)\\}_{t=0}^1\\) for \\(i=1,...,N\\)\nTreatment \\(D_{it} \\in \\{0,1\\}\\),\n\n\\(D_{i0} = 0\\) \\(\\forall i\\)\n\\(D_{i1} = 1\\) for some, \\(0\\) for others\n\nObserve \\(y_{it} = y_{it}(0)(1-D_{it}) + D_{it} y_{it}(1)\\)"
  },
  {
    "objectID": "did.html#identification",
    "href": "did.html#identification",
    "title": "Difference in Differences",
    "section": "Identification",
    "text": "Identification\n\nAverage treatment effect on the treated: \\[\n\\begin{align*}\nATT & = \\Er[y_{i1}(1) - \\color{red}{y_{i1}(0)} | D_{i1} = 1] \\\\\n& = \\Er[y_{i1}(1) - y_{i0}(0) | D_{i1} = 1] - \\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) | D_{i1}=1] \\\\\n& \\text{ assume } \\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) | D_{i1}=1] =  \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0] \\\\\n& = \\Er[y_{i1}(1) - y_{i0}(0) | D_{i1} = 1] - \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0] \\\\\n& = \\Er[y_{i1} - y_{i0} | D_{i1}=1, D_{i0}=0] - \\Er[y_{i1} - y_{i0} | D_{i1}=0, D_{i0}=0]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "did.html#important-assumptions",
    "href": "did.html#important-assumptions",
    "title": "Difference in Differences",
    "section": "Important Assumptions",
    "text": "Important Assumptions\n\nNo anticipation: \\(D_{i1}=1\\) does not affect \\(y_{i0}\\)\n\nbuilt into the potential outcomes notation we used, relax by allowing potential outcomes given sequence of \\(D\\), i.e. \\(y_{it}(D_{i0},D_{i1})\\)\n\nParallel trends: \\(\\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) |D_{i1}=1,D_{i0}=0] = \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0], D_{i0}=0]\\)\n\nnot invariant to tranformations of \\(y\\)"
  },
  {
    "objectID": "did.html#estimation",
    "href": "did.html#estimation",
    "title": "Difference in Differences",
    "section": "Estimation",
    "text": "Estimation\n\n\nPlugin: \\[\n\\widehat{ATT} = \\frac{ \\sum_{i=1}^n (y_{i1} - y_{i0})D_{i1}(1-D_{i0})}{\\sum_{i=1}^n D_{i1}(1-D_{i0})} -  \\frac{ \\sum_{i=1}^n (y_{i1} - y_{i0})(1-D_{i1})(1-D_{i0})}{\\sum_{i=1}^n (1-D_{i1})(1-D_{i0})}\n\\]\nRegression: \\[\ny_{it} = \\delta_t + \\alpha 1\\{D_{i1}=1\\} + \\beta D_{it} + \\epsilon_{it}\n\\] then \\(\\hat{\\beta} = \\widehat{ATT}\\)"
  },
  {
    "objectID": "did.html#visualizing-difference-in-differences",
    "href": "did.html#visualizing-difference-in-differences",
    "title": "Difference in Differences",
    "section": "Visualizing Difference in Differences",
    "text": "Visualizing Difference in Differences\n\npoa_before = data.query(\"poa==1 & jul==0\")[\"deposits\"].mean()\npoa_after = data.query(\"poa==1 & jul==1\")[\"deposits\"].mean()\nfl_before = data.query(\"poa==0 & jul==0\")[\"deposits\"].mean()\nfl_after = data.query(\"poa==0 & jul==1\")[\"deposits\"].mean()\nplt.figure(figsize=(10,5))\nplt.plot([\"May\", \"Jul\"], [fl_before, fl_after], label=\"FL\", lw=2)\nplt.plot([\"May\", \"Jul\"], [poa_before, poa_after], label=\"POA\", lw=2)\n\nplt.plot([\"May\", \"Jul\"], [poa_before, poa_before+(fl_after-fl_before)],\n         label=\"Counterfactual\", lw=2, color=\"C2\", ls=\"-.\")\n\nplt.legend();"
  },
  {
    "objectID": "did.html#estimation-via-regression",
    "href": "did.html#estimation-via-regression",
    "title": "Difference in Differences",
    "section": "Estimation via Regression",
    "text": "Estimation via Regression\n\nsmf.ols('deposits ~ poa*jul', data=data).fit().summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n171.6423\n2.363\n72.625\n0.000\n167.009\n176.276\n\n\npoa\n-125.6263\n4.484\n-28.015\n0.000\n-134.418\n-116.835\n\n\njul\n34.5232\n3.036\n11.372\n0.000\n28.571\n40.475\n\n\npoa:jul\n6.5246\n5.729\n1.139\n0.255\n-4.706\n17.755"
  },
  {
    "objectID": "did.html#further-topics",
    "href": "did.html#further-topics",
    "title": "Difference in Differences",
    "section": "Further Topics",
    "text": "Further Topics\n\nMore periods, more groups\nCovariates\nPre-trends"
  },
  {
    "objectID": "did.html#reading",
    "href": "did.html#reading",
    "title": "Difference in Differences",
    "section": "Reading",
    "text": "Reading\n\nChapter 13 of Facure (2022)"
  },
  {
    "objectID": "did.html#references",
    "href": "did.html#references",
    "title": "Difference in Differences",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html."
  }
]