{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON526: Quantitative Economics with Data Science Applications\n",
        "\n",
        "Foundations of Numerical Linear Algebra\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "## Going Beyond “`reg y x, robust`”\n",
        "\n",
        "-   Data science, econometrics, and macroeconomics are built on linear\n",
        "    algebra.\n",
        "\n",
        "-   Numerical linear algebra has all sorts of pitfalls, which become\n",
        "    more critical as we scale up to larger problems.\n",
        "\n",
        "-   Speed differences in choosing better algorithms can be orders of\n",
        "    magnitude.\n",
        "\n",
        "-   Crucial to know what goes on under-the-hood in Stata/R/python\n",
        "    packages for applied work, even if you don’t implement it yourself.\n",
        "\n",
        "-   Material here is related to\n",
        "\n",
        "    -   [QuantEcon\n",
        "        Python](https://python.quantecon.org/linear_algebra.html)\n",
        "    -   [QuantEcon Data\n",
        "        Science](https://datascience.quantecon.org/scientific/applied_linalg.html)\n",
        "    -   [A First Course in Quantitative Economics with\n",
        "        Python](https://intro.quantecon.org/)\n",
        "\n",
        "## Packages\n",
        "\n",
        "This section uses the following packages:"
      ],
      "id": "b102c269-f03b-4796-8ab1-f1ff22c05af1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from numpy.linalg import cond, matrix_rank, norm\n",
        "from scipy.linalg import inv, solve, det, eig, lu, eigvals\n",
        "from scipy.linalg import solve_triangular, eigvalsh, cholesky"
      ],
      "id": "0afb4a77"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Computational Complexity\n",
        "\n",
        "**Big-O Notation**\n",
        "\n",
        "For a function $f(N)$ and a positive constant $C$, we say $f(N)$ is\n",
        "$O(g(N))$, if there exist positive constants $C$ and $N_0$ such that:\n",
        "\n",
        "$$\n",
        "0 \\leq f(N) \\leq C \\cdot g(N) \\quad \\text{for all } N \\geq N_0\n",
        "$$\n",
        "\n",
        "-   Often crucial to know how problems scale asymptotically (as\n",
        "    $N\\to\\infty$)\n",
        "-   Caution! This is only an asymptotic limit, and can be misleading for\n",
        "    small $N$\n",
        "    -   $f_1(N) = N^3 + N$ is $O(N^3)$\n",
        "    -   $f_2(N) = 1000 N^2 + 3 N$ is $O(N^2)$\n",
        "    -   For roughly $N>1000$ use $f_2$ algorithm, otherwise $f_1$\n",
        "\n",
        "## Examples of Computational Complexity\n",
        "\n",
        "-   Simple examples:\n",
        "    -   $x \\cdot y = \\sum_{n=1}^N x_n y_n$ is $O(N)$ since it requires\n",
        "        $N$ multiplications and additions\n",
        "    -   $A x$ for $A\\in\\mathbb{R}^{N\\times N},x\\in\\mathbb{R}^N$ is\n",
        "        $O(N^2)$ since it requires $N$ dot products, each $O(N)$\n",
        "\n",
        "## Numerical Precision\n",
        "\n",
        "**Machine Epsilon**\n",
        "\n",
        "For a given datatype, $\\epsilon$ is defined as\n",
        "$\\epsilon = \\min_{\\delta > 0} \\left\\{ \\delta : 1 + \\delta > 1 \\right\\}$\n",
        "\n",
        "-   Computers have finite precision. 64-bit typical, but 32-bit on GPUs"
      ],
      "id": "db08c48d-da80-4b92-bb12-3ec8f430cdfa"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine epsilon for float64 = 2.220446049250313e-16\n",
            "1 + eps/2 == 1? True\n",
            "machine epsilon for float32 = 1.1920928955078125e-07"
          ]
        }
      ],
      "source": [
        "print(f\"machine epsilon for float64 = {np.finfo(float).eps}\")\n",
        "print(f\"1 + eps/2 == 1? {1.0 + 1.1e-16 == 1.0}\")\n",
        "print(f\"machine epsilon for float32 = {np.finfo(np.float32).eps}\")"
      ],
      "id": "7768ca71"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Linear Algebra\n",
        "\n",
        "## Norms\n",
        "\n",
        "-   Common measure of size is the Euclidean norm, or $L^2$ norm for\n",
        "    $x \\in \\mathbb{R}^2$\n",
        "-   Complexity is $O(N)$, square $N$ times then $N$ additions $$\n",
        "    ||x||_2 = \\sqrt{\\sum_{n=1}^N x_n^2}\n",
        "    $$"
      ],
      "id": "9d97203a-6d05-4f21-875d-085d3c2b9f36"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7416573867739413\n",
            "3.7416573867739413\n",
            "3.7416573867739413\n",
            "||x||_2^2 = 14.0 = 14 = 14"
          ]
        }
      ],
      "source": [
        "x = np.array([1, 2, 3]) # Calculating different ways (in order of preference)\n",
        "print(np.sqrt(sum(xval**2 for xval in x))) # manual with comprehensions\n",
        "print(np.sqrt(np.sum(np.square(x)))) # broadcasts\n",
        "print(norm(x)) # built-in to numpy norm(x, ord=2) alternatively\n",
        "print(f\"||x||_2^2 = {norm(x)**2} = {x.T @ x} = {np.dot(x, x)}\")"
      ],
      "id": "2432e455"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving Systems of Equations\n",
        "\n",
        "-   Solving $A x = b$ for $x$ is equivalent $A^{-1} A x = A^{-1} b$\n",
        "-   Then since $A^{-1}A = I$, and $I x = x$, we have $x = A^{-1} b$\n",
        "-   Careful since matrix algebra is not commutative!"
      ],
      "id": "075c7945-4f93-4b98-8a09-f23bfa39def7"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-1.,  1.])"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[0, 2], [3, 4]]) # or ((0, 2), (3, 4))\n",
        "b = np.array([2,1])  # Column vector\n",
        "x = solve(A, b)  # Solve Ax = b for x\n",
        "x"
      ],
      "id": "1b074dcd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the Inverse Directly\n",
        "\n",
        "-   Can replace the `solve` with a calculation of an inverse\n",
        "-   But it can be slower or less accurate than solving the system\n",
        "    directly"
      ],
      "id": "06eedb2a-0ffb-401c-8dcd-f1906d35f3ba"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-1.,  1.])"
            ]
          }
        }
      ],
      "source": [
        "A_inv = inv(A)\n",
        "A_inv @ b # i.e, A^{-1} * b"
      ],
      "id": "a24822ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Combinations\n",
        "\n",
        "We can think of solving a system as finding the linear combination of\n",
        "columns of $A$ that equal $b$"
      ],
      "id": "79fcfcf0-401b-43c0-bdf5-27ffd3717f53"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b = [2 1], b_star = [2. 1.]"
          ]
        }
      ],
      "source": [
        "b_star = x[0] * A[:, 0] + x[1] * A[:, 1] # using x solution\n",
        "print(f\"b = {b}, b_star = {b_star}\")"
      ],
      "id": "b8663976"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Column Space and Rank\n",
        "\n",
        "-   The column space of a matrix represents all possible linear\n",
        "    combinations of its columns.\n",
        "-   It forms a basis for the space of solutions when solving systems of\n",
        "    linear equations represented by the matrix\n",
        "-   The rank of a matrix is the dimension of its column space"
      ],
      "id": "a50fb67c-a95a-403d-903a-529ee9f610af"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "2"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[0, 2], [3, 4]])\n",
        "matrix_rank(A)"
      ],
      "id": "ed99d4fe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hence, can solve $A x = b$ for any $b \\in \\mathbb{R}^2$ since the column\n",
        "space is the entire space $\\mathbb{R}^2$\n",
        "\n",
        "## Singular Matrices\n",
        "\n",
        "On the other hand, note"
      ],
      "id": "52789f2c-cf76-4756-8362-1bd9a2f60470"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "1"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[1, 2],\n",
        "              [2, 4]])\n",
        "matrix_rank(A)"
      ],
      "id": "9fdac602"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can only solve $A x = b$ for\n",
        "$b \\propto \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\propto \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$\n",
        "\n",
        "## Checking Singularity"
      ],
      "id": "b18f4541-46ce-4bed-b892-8e3f9cbc0fab"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "Matrix is singular (non-invertible)."
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2], [2, 4]])\n",
        "# An (expensive) way to check if A is singular is if det(A) = 0\n",
        "print(det(A) == 0.0)\n",
        "print(matrix_rank(A) != A.shape[0]) # or check rank\n",
        "# Check before inverting or use exceptions\n",
        "try:\n",
        "    inv(A)\n",
        "    print(\"Matrix is not singular (invertible).\")\n",
        "except np.linalg.LinAlgError:\n",
        "    print(\"Matrix is singular (non-invertible).\")"
      ],
      "id": "e7609b1f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determinant is Not Scale Invariant\n",
        "\n",
        "-   Reminder: numerical precision in calculations makes it hard to\n",
        "    compare to zero\n",
        "-   The determinate is useful but depends on the scale of the matrix\n",
        "-   A more robust alternative is the condition number (more next\n",
        "    lecture)"
      ],
      "id": "beadc0e3-768a-45a2-a523-9e203113c707"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "det(A)=-1e-08, det(K*A)=-100\n",
            "cond(A)=1e+09, cond(K*A)=1e+09,\n",
            "det(inv(A))=-1e+08, cond(inv(A))=1e+09"
          ]
        }
      ],
      "source": [
        "eps, K = 1e-8, 100000\n",
        "A = np.array([[1, 2], [1 + eps, 2 + eps]])\n",
        "print(f\"det(A)={det(A):.5g}, det(K*A)={det(K*A):.5g}\")\n",
        "print(f\"cond(A)={cond(A):.5g}, cond(K*A)={cond(A):.5g},\")\n",
        "print(f\"det(inv(A))={det(inv(A)):.5g}, cond(inv(A))={cond(inv(A)):.5g}\")"
      ],
      "id": "ea3df882"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solving Linear Systems of Equations\n",
        "\n",
        "## Solving Systems with Multiple RHS\n",
        "\n",
        "-   Inverse is nice because you can reuse the $A^{-1}$ to solve\n",
        "    $A x = b$ for many $b$\n",
        "-   However, you can do this with `solve` as well\n",
        "-   Or can reuse LR factorizations (discussed next)"
      ],
      "id": "d2c6f1eb-4d5c-45af-99c3-69ad5bf7cb75"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.         -1.33333333]\n",
            " [ 1.          1.5       ]]\n",
            "Checking: A*[-1.  1.] = [2. 1.] = [2 1], column of B"
          ]
        }
      ],
      "source": [
        "A = np.array([[0, 2], [3, 4]])\n",
        "B = np.array([[2,3], [1,2]])  # [2,1] and [3,2] as columns\n",
        "# or: B = np.column_stack([np.array([2, 1]),np.array([3,2])])\n",
        "X = solve(A, B)  # Solve AX = B for X\n",
        "print(X)\n",
        "print(f\"Checking: A*{X[:,0]} = {A@X[:, 0]} = {B[:,0]}, column of B\")"
      ],
      "id": "ead83b78"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LU(P) Decompositions\n",
        "\n",
        "-   We can “factor” any square $A$ into $P A = L U$ for triangular $L$\n",
        "    and $U$. Invertible can have $A = L U$, called the LU decomposition.\n",
        "    “P” is for partial-pivoting\n",
        "-   Singular matrices may not have full-rank $L$ or $U$ matrices"
      ],
      "id": "79b24004-f5ec-49db-874e-6fba0a14c471"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L*U =\n",
            "[[2. 4.]\n",
            " [1. 2.]]\n",
            "P*A =\n",
            "[[2. 4.]\n",
            " [1. 2.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2], [2, 4]])\n",
        "P, L, U = lu(A)\n",
        "print(f\"L*U =\\n{L @ U}\")\n",
        "print(f\"P*A =\\n{P @ A}\")"
      ],
      "id": "8199d2c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## P, U, and L\n",
        "\n",
        "The $P$ matrix is a permutation matrix of “pivots” the others are\n",
        "triangular"
      ],
      "id": "c5c9a7e0-1311-4208-8c24-6be83296ab38"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P =\n",
            "[[0. 1.]\n",
            " [1. 0.]]\n",
            "L =\n",
            "[[1.  0. ]\n",
            " [0.5 1. ]]\n",
            "U =\n",
            "[[2. 4.]\n",
            " [0. 0.]]"
          ]
        }
      ],
      "source": [
        "print(f\"P =\\n{P}\")\n",
        "print(f\"L =\\n{L}\")\n",
        "print(f\"U =\\n{U}\")"
      ],
      "id": "88e0910a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LU Decompositions and Systems of Equations\n",
        "\n",
        "-   Pivoting is typically implied when talking about “LU”\n",
        "-   Used in the default `solve` algorithm (without more structure)\n",
        "-   Solving systems of equations with triangular matrices: for\n",
        "    $A x = L U x = b$\n",
        "    1.  Define $y = U x$\n",
        "    2.  Solve $L y = b$ for $y$ and $U x = y$ for $x$\n",
        "-   Since both are triangular, process is $O(N^2)$ (but LU itself\n",
        "    $O(N^3)$)\n",
        "-   Could be used to find `inv`\n",
        "    -   $A = L U$ then $A A^{-1} = I = L U A^{-1} = I$\n",
        "    -   Solve for $Y$ in $L Y = I$, then solve $U A^{-1} = Y$\n",
        "-   Tight connection to textbook Gaussian elimination (including\n",
        "    pivoting)\n",
        "\n",
        "## LU for Non-Singular Matrices"
      ],
      "id": "0809ba67-1283-4cf5-8209-d0e0c902e253"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L*U =\n",
            "[[3. 4.]\n",
            " [1. 2.]]\n",
            "P*A =\n",
            "[[3. 4.]\n",
            " [1. 2.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2], [3, 4]])\n",
        "P, L, U = lu(A)\n",
        "print(f\"L*U =\\n{L @ U}\")\n",
        "print(f\"P*A =\\n{P @ A}\")"
      ],
      "id": "23d79354"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L, U, P"
      ],
      "id": "7be8467b-5a15-4e42-a5c9-262027efb004"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P =\n",
            "[[0. 1.]\n",
            " [1. 0.]]\n",
            "L =\n",
            "[[1.         0.        ]\n",
            " [0.33333333 1.        ]]\n",
            "U =\n",
            "[[3.         4.        ]\n",
            " [0.         0.66666667]]"
          ]
        }
      ],
      "source": [
        "print(f\"P =\\n{P}\")\n",
        "print(f\"L =\\n{L}\")\n",
        "print(f\"U =\\n{U}\")"
      ],
      "id": "d820c8d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backwards Substitution Example\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "U x &= b\\\\\n",
        "U &\\equiv \\begin{bmatrix}\n",
        "3 & 1 \\\\\n",
        "0 & 2 \\\\\n",
        "\\end{bmatrix}, \\quad b = \\begin{bmatrix}\n",
        "7 \\\\\n",
        "2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Solving bottom row for $x_2$\n",
        "\n",
        "$$\n",
        "2 x_2 = 2,\\quad x_2 = 1\n",
        "$$\n",
        "\n",
        "Move up a row, solving for $x_1$, substituting for $x_2$\n",
        "\n",
        "$$\n",
        "3 x_1 + 1 x_2 = 7,\\quad 3 x_1 + 1 \\times 1 =  7,\\quad x_1 = 2\n",
        "$$\n",
        "\n",
        "Generalizes to many rows. For $L$ it is “forward substitution”\n",
        "\n",
        "## Use Triangular Structure if Possible\n",
        "\n",
        "-   Triangular matrices of size $N$ can be solved with back substitution\n",
        "    in $O(N^2)$\n",
        "-   Is $O(N^2)$ good or bad? Beats, $O(N^3)$ typical of general methods"
      ],
      "id": "99ed39ad-550e-40c9-9a28-2826f73e9acf"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([2., 1.])"
            ]
          }
        }
      ],
      "source": [
        "U = np.array([[3, 1],\n",
        "              [0, 2]])\n",
        "b = np.array([7,2])\n",
        "solve(U,b) # works, but internally does an LU which is O(N^3)\n",
        "solve_triangular(U, b, lower=False) # fast O(N^2)"
      ],
      "id": "af91ee66"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Symmetric Matrix Structure\n",
        "\n",
        "Another common matrix type are symmetric, $A = A^{T}$"
      ],
      "id": "3bf49a65-214d-416d-ad48-fd6fb3efa231"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([-3.,  2.])"
            ]
          }
        }
      ],
      "source": [
        "A = np.array([[1, 2], [2, 5]]) # also posdef, not singular\n",
        "b = np.array([1,4])\n",
        "# With scipy 1.11.3 check with scipy.linalg.issymmetric(A)\n",
        "solve(A, b, assume_a=\"sym\") # could also use \"pos\" since positive definite"
      ],
      "id": "d23956c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positive Definite Matrices\n",
        "\n",
        "-   A symmetric matrix $A$ is positive definite if $x^T A x > 0$ for all\n",
        "    $x \\neq 0$\n",
        "-   Useful in many areas, such as covariance matrices. Example"
      ],
      "id": "a5cc0bba-ae90-457f-8b07-37ded794c536"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x^T A x = 5"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2], [2, 5]])\n",
        "x = np.array([0, 1]) # can't really check for all x\n",
        "print(f\"x^T A x = {x.T @ A @ x}\")"
      ],
      "id": "e38ac69c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Example of a symmetric matrix that is not positive definite"
      ],
      "id": "f77a30f3-0c5d-4d9a-b0ff-f79cc4de8c86"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x^T A x = 0"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2], [2, 0]])\n",
        "print(f\"x^T A x = {x.T @ A @ x}\")  # one counterexample is enough"
      ],
      "id": "2e540b2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   We can check these with eigenvalues\n",
        "\n",
        "## Cholesky Decomposition\n",
        "\n",
        "-   For symmetric positive definite matrices: $L = U^T$’\n",
        "-   Called a Cholesky decomposition: $A = L L^T$ for a lower triangular\n",
        "    matrix $L$.\n",
        "-   Equivalently, could find A = $U^T U$ for an upper triangular matrix\n",
        "    $U$"
      ],
      "id": "54ed2d4d-0bbb-4bfd-a2dd-18121569a19c"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0.]\n",
            " [2. 1.]]\n",
            "L*L^T =\n",
            "[[1. 2.]\n",
            " [2. 5.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2], [2, 5]])\n",
        "L = cholesky(A, lower=True) # cholesky also defined for upper=True\n",
        "print(L)\n",
        "print(f\"L*L^T =\\n{L @ L.T}\")"
      ],
      "id": "168d088e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving Positive Definite Systems"
      ],
      "id": "e9f01e1b-5bab-4351-b424-1b3f510c7975"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.  2.]\n",
            "[-3.  2.]"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 2], [2, 5]])\n",
        "b = np.array([1,4])\n",
        "print(solve(A, b, assume_a=\"pos\")) # uses cholesky internally\n",
        "\n",
        "L = cholesky(A, lower=True)\n",
        "y = solve_triangular(L, b, lower=True)\n",
        "x = solve_triangular(L.T, y, lower=False)\n",
        "print(x)"
      ],
      "id": "24d93f8b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cholesky for Covariance Matrices\n",
        "\n",
        "-   Covariance matrices are positive-definite, semi-definite if\n",
        "    degenerate\n",
        "-   Key property of Gaussian random variables:\n",
        "    -   $X \\sim N(\\mu, \\Sigma)$ for\n",
        "        $\\mu \\in \\mathbb{R}^N, \\Sigma \\in \\mathbb{R}^{N \\times N}$\n",
        "    -   $X = \\mu + A Z$ for $Z \\sim N(0_N, I_N)$ where $A A^T = \\Sigma$\n",
        "-   That is, $A$ is the Cholesky decomposition of the covariance matrix\n",
        "\n",
        "## Matrices as Linear Transformations\n",
        "\n",
        "-   Recall: for $x \\in \\mathbb{R}^N$ we should think of a $f(x) = A x$\n",
        "    for $A\\in\\mathbb{R}^{M \\times N}$ as a linear transformation from\n",
        "    $\\mathbb{R}^N$ to $\\mathbb{R}^M$\n",
        "    -   Definition of Linear: $f(a x_1 + b x_2) = a f(x_1) + b f(x_2)$\n",
        "        for scalar $a,b$\n",
        "-   Similarly, the $y = f(x) = A x$ then $f^{-1}(y) = A^{-1} y$ goes\n",
        "    from $\\mathbb{R}^M$ to $\\mathbb{R}^N$\n",
        "    -   If the matrix is square and invertible, we can go back and forth\n",
        "        without losing information (i.e., bijective). Otherwise we may\n",
        "        be projected onto a lower-dimensional “manifold”.\n",
        "\n",
        "## Norms and Linear Transformations\n",
        "\n",
        "-   The vector norm $||x||_2$ is an important feature in many\n",
        "    applications\n",
        "\n",
        "    -   Hence $||f(x)||_2 = ||A x||_2$ frequently comes up in economics\n",
        "        and datascience\n",
        "    -   e.g. linear regression is written as minimizing a vector norm\n",
        "\n",
        "    $$\n",
        "    \\min_{\\beta} ||y - X \\beta||_2\n",
        "    $$\n",
        "\n",
        "-   Matrix structure or decompositions of $A$ help us better understand\n",
        "    the $f(x)$ mapping\n",
        "\n",
        "## Orthogonal Matrices\n",
        "\n",
        "-   A square matrix $Q$ is **orthogonal** if: $Q^{-1} = Q^T$, and hence\n",
        "    $Q^T Q = Q Q^T = I$\n",
        "    -   For orthogonal $Q$, $f(x) = Q x$ is interpreted as rotating $x$\n",
        "        without stretching\n",
        "    -   $y = f(x) = Q x$ then $f^{-1}(y) = Q^{-1} y = Q^T y$ is rotating\n",
        "        $y$ back\n",
        "    -   Columns are orthonormal:\n",
        "        $Q = \\begin{bmatrix}q_1 |& \\ldots & | q_N\\end{bmatrix}$ then\n",
        "        -   $q_i \\cdot q_j = 0$ for $i \\neq j$ and $q_i \\cdot q_i = 1$\n",
        "    -   Rotation means the length doesn’t change: $||Q x||_2 = ||x||_2$\n",
        "    -   Transformations which preserve norms are central in many\n",
        "        applications within data science, ML, and economics - especially\n",
        "        in high-dimensions\n",
        "\n",
        "# Eigenvalues and Eigenvectors\n",
        "\n",
        "## Eigenvalues and Eigenvectors\n",
        "\n",
        "-   For a square $A$, an eigenvector $x$ and eigenvalue $\\lambda$\n",
        "    satisfy $$\n",
        "    A x = \\lambda x\n",
        "    $$\n",
        "\n",
        "-   $A\\in\\mathbb{R}^{N\\times N}$ has $N$ eigenvalue/eigenvector pairs,\n",
        "    possible multiplicity of $\\lambda$\n",
        "\n",
        "-   Intuition: $x$ is a direction $A x \\propto x$ and $\\lambda$ says how\n",
        "    much it “stretches”\n",
        "\n",
        "## Properties of Eigenvalues and Eigenvectors\n",
        "\n",
        "-   For any eigenvector $x$ and scalar $c$ then $c x \\propto A x$ as\n",
        "    well\n",
        "-   Symmetric matrices have real eigenvalues and orthogonal\n",
        "    eigenvectors. i.e. $x_1 \\cdot x_2 = 0$ for $x_1 \\neq x_2$\n",
        "    eigenvectors. Complex in general\n",
        "-   Singular if and only if it has an eigenvalue of zero\n",
        "-   Positive (semi)definite if and only if all eigenvalues are strictly\n",
        "    (weakly) positive\n",
        "-   Diagonal matrix has eigenvalues as its diagonal\n",
        "-   Triangular matrix has eigenvalues as its diagonal\n",
        "\n",
        "## Positive Definite and Eigenvalues\n",
        "\n",
        "You cannot check $x^T A x > 0$ for all $x$. Check if “stretching” is\n",
        "positive"
      ],
      "id": "b87ff0e6-a0c0-4eae-bc76-83c879b3871a"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.23606798  4.23606798]\n",
            "pos-def? False\n",
            "pos-semi-def? False"
          ]
        }
      ],
      "source": [
        "A = np.array([[3, 1], [2, 1]])\n",
        "# A_eigs = np.real(eigvals(A)) # symmetric matrices have real eigenvalues\n",
        "A_eigs = eigvalsh(A) # specialized for symmetric/hermitian matrices\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "5b5d7f18"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positive Semi-Definite Matrices **May** Have a Zero Eigenvalue\n",
        "\n",
        "The simplest positive-semi-definite (but not posdef) matrix is"
      ],
      "id": "d3e22b8c-1f83-405d-a552-7af859fd7de2"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n",
            "pos-def? False\n",
            "pos-semi-def? True"
          ]
        }
      ],
      "source": [
        "A_eigs = eigvalsh(np.array([[1, 0], [0, 0]]))\n",
        "print(A_eigs)\n",
        "is_positive_definite = np.all(A_eigs > 0)\n",
        "is_positive_semi_definite = np.all(A_eigs >= 0) # or eigvals(A) >= -eps\n",
        "print(f\"pos-def? {is_positive_definite}\")\n",
        "print(f\"pos-semi-def? {is_positive_semi_definite}\")"
      ],
      "id": "d35b053f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigen Decomposition\n",
        "\n",
        "-   For square, symmetric, non-singular matrix $A$ factor into\n",
        "\n",
        "$$\n",
        "A = Q \\Lambda Q^{-1}\n",
        "$$\n",
        "\n",
        "-   $Q$ is a matrix of eigenvectors, $\\Lambda$ is a diagonal matrix of\n",
        "    paired eigenvalues\n",
        "-   For symmetric matrices, the eigenvectors are orthogonal and\n",
        "    $Q^{-1} Q = Q^T Q = I$ which form an orthonormal basis\n",
        "-   Orthogonal matrices can be thought of as rotations without\n",
        "    stretching\n",
        "-   More general matrices all have a Singular Value Decomposition (SVD)\n",
        "-   With symmetric $A$, an interpretation of $A x$ is that we can first\n",
        "    rotate $x$ into the $Q$ basis, then stretch by $\\Lambda$, then\n",
        "    rotate back\n",
        "\n",
        "## Eigendecompositions and Matrix Powers\n",
        "\n",
        "-   Can be used to find $A^t$ for large $t$ (e.g. for Markov chains)\n",
        "    -   $P^t$, i.e. $P \\cdot P \\cdot \\ldots \\cdot P$ for $t$ times\n",
        "    -   $P = Q \\Lambda Q^{-1}$ then $P^t = Q \\Lambda^t Q^{-1}$ where\n",
        "        $\\Lambda^t$ is just the pointwise power\n",
        "-   Related tools such as SVD can help with dimensionality reduction\n",
        "\n",
        "## Spectral/Eigendecomposition of Symmetric Matrix Example"
      ],
      "id": "15aa072f-5480-4ccc-9f84-d605f3577a4a"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvectors are column-by-column in Q =\n",
            "[[-0.85065081 -0.52573111]\n",
            " [ 0.52573111 -0.85065081]]\n",
            "eigenvalues are in Lambda = [1.38196601+0.j 3.61803399+0.j]\n",
            "Q Lambda Q^T =\n",
            "[[2. 1.]\n",
            " [1. 3.]]"
          ]
        }
      ],
      "source": [
        "A = np.array([[2, 1], [1, 3]])\n",
        "Lambda, Q = eig(A)\n",
        "print(f\"eigenvectors are column-by-column in Q =\\n{Q}\")\n",
        "print(f\"eigenvalues are in Lambda = {Lambda}\")\n",
        "print(f\"Q Lambda Q^T =\\n{Q @ np.diag(np.real(Lambda)) @ Q.T}\")"
      ],
      "id": "3ae5c3ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectral Radius is Maximum Absolute Eigenvalue\n",
        "\n",
        "-   If any $\\lambda \\in \\Lambda$ are $> 1$ can see this would explode\n",
        "-   Useful for seeing if iteration $x_{t+1} = A x_t$ from a $x_0$\n",
        "    explodes\n",
        "-   The **spectral radius** of matrix $A$ is\n",
        "\n",
        "$$\n",
        "  \\rho(A) = \\max_{\\lambda \\in \\Lambda}|\\lambda|\n",
        "  $$\n",
        "\n",
        "# Least Squares and the Normal Equations\n",
        "\n",
        "## Least Squares\n",
        "\n",
        "Given a matrix $X \\in \\mathbb{R}^{N \\times M}$ and a vector\n",
        "$y \\in \\mathbb{R}^N$, we want to find $\\beta \\in \\mathbb{R}^M$ such that\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\min_{\\beta} &||y - X \\beta||^2, \\text{ that is,}\\\\\n",
        "\\min_{\\beta} &\\sum_{n=1}^N \\frac{1}{N}(y_n - X_n \\cdot \\beta)^2\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where $X_n$ is n’th row. Take FOCS and rearrange to get\n",
        "\n",
        "$$\n",
        "(X^T X) \\beta =  X^T y\n",
        "$$\n",
        "\n",
        "## Solving the Normal Equations\n",
        "\n",
        "-   The $X$ is often referred to as the “design matrix”. $X^T X$ as the\n",
        "    Gram matrix\n",
        "\n",
        "-   Can form $A = X^T X$ and $b = X^T y$ and solve $A \\beta = b$.\n",
        "\n",
        "    -   Or invert $X^T X$ to get\n",
        "\n",
        "    $$\n",
        "    \\beta = (X^T X)^{-1} X^T y\n",
        "    $$\n",
        "\n",
        "    -   Note that $X^T X$ is symmetric and, if $X$ is full-rank,\n",
        "        positive definite\n",
        "\n",
        "## Solving Regression Models in Practice\n",
        "\n",
        "-   In practice, use the `lstsq` function in scipy\n",
        "    -   It uses better algorithms using eigenvectors. More stable (see\n",
        "        next lecture on conditioning)\n",
        "    -   One algorithm uses another factoring, the QR decomposition\n",
        "    -   There, $X = Q R$ for $Q$ orthogonal and $R$ upper triangular.\n",
        "        See [QR\n",
        "        Decomposition](https://python.quantecon.org/qr_decomp.html) for\n",
        "        more\n",
        "-   Better yet, for applied work use higher-level libraries like\n",
        "    `statsmodels` (integrates well with `pandas` and `seaborn`)\n",
        "    -   See [statsmodels\n",
        "        docs](https://www.statsmodels.org/dev/example_formulas.html) for\n",
        "        R-style notation\n",
        "    -   See [QuantEcon OLS Notes](https://python.quantecon.org/ols.html)\n",
        "        for more.\n",
        "\n",
        "## Example of LLS using Scipy"
      ],
      "id": "0361260e-c1f1-4abd-ba21-8d88e9893716"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta =\n",
            " [-1.85409462 -0.34758154  0.13924102  0.2247039  -0.665208  ]\n",
            "beta_hat =\n",
            "[-1.85390862 -0.34712613  0.14483963  0.21735707 -0.6622151 ]"
          ]
        }
      ],
      "source": [
        "N, M = 100, 5\n",
        "X = np.random.randn(N, M)\n",
        "beta = np.random.randn(M)\n",
        "y = X @ beta + 0.05 * np.random.randn(N)\n",
        "beta_hat, residuals, rank, s = scipy.linalg.lstsq(X, y)\n",
        "print(f\"beta =\\n {beta}\\nbeta_hat =\\n{beta_hat}\")"
      ],
      "id": "b6a1a05d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving using the Normal Equations\n",
        "\n",
        "Or we can solve it directly. Provide matrix structure (so it can use a\n",
        "Cholesky)"
      ],
      "id": "028223be-f2aa-418a-bd75-797921bea7ca"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta =\n",
            " [-1.85409462 -0.34758154  0.13924102  0.2247039  -0.665208  ]\n",
            "beta_hat =\n",
            "[-1.85390862 -0.34712613  0.14483963  0.21735707 -0.6622151 ]"
          ]
        }
      ],
      "source": [
        "beta_hat = solve(X.T @ X, X.T @ y, assume_a=\"pos\")\n",
        "print(f\"beta =\\n {beta}\\nbeta_hat =\\n{beta_hat}\")"
      ],
      "id": "67f865bf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collinearity in “Tall” Matrices\n",
        "\n",
        "-   Tall $\\mathbb{R}^{N\\times M}$ “design matrices” have $N > M$ and are\n",
        "    “overdetermined”\n",
        "-   The rank of a matrix is full rank if all columns are linearly\n",
        "    independent\n",
        "-   You can only identify $M$ parameters with $M$ linearly independent\n",
        "    columns"
      ],
      "id": "78691592-dda6-4dd8-9b80-30db2ca62544"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rank(X) = 2, rank(X_col) = 1"
          ]
        }
      ],
      "source": [
        "X = np.array([[1, 2], [2, 5], [3, 7]]) # 3 observations, 2 variables\n",
        "X_col = np.array([[1, 2], [2, 4], [3, 6]]) # all proportional\n",
        "print(f\"rank(X) = {matrix_rank(X)}, rank(X_col) = {matrix_rank(X_col)}\")"
      ],
      "id": "088c7bee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collinearity and Estimation\n",
        "\n",
        "-   If $X$ is not full rank, then $X^T X$ is not invertible. For\n",
        "    example:"
      ],
      "id": "c9085c2e-a593-472a-bd19-24647909db1b"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cond(X'*X)=2819.332978639814, cond(X_col'*X_col)=1.1014450683078442e+16"
          ]
        }
      ],
      "source": [
        "print(f\"cond(X'*X)={cond(X.T@X)}, cond(X_col'*X_col)={cond(X_col.T@X_col)}\")"
      ],
      "id": "6e86ff9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Note that when you start doing operations on matrices, numerical\n",
        "    error creeps in, so you will not get an exact number\n",
        "-   The rule-of-thumb with condition numbers is that if it is\n",
        "    $1\\times 10^k$ then you lose about $k$ digits of precision. So this\n",
        "    effectively means it is singular\n",
        "-   Given the singular matrix, this means a continuum of $\\beta$ will\n",
        "    solve the problem\n",
        "\n",
        "## `lstsq` Solves it? Careful on Interpretation!\n",
        "\n",
        "-   Since $X_{col}^T X_{col}$ is singular, we cannot use\n",
        "    `solve(X.T@X, y)`\n",
        "-   But what about `lstsq` methods?\n",
        "-   As you will see, this gives an answer. Interpretation is hard\n",
        "-   The key is that in the case of non-full rank, you cannot identify\n",
        "    individual parameters\n",
        "    -   Related to “Identification” in econometrics\n",
        "    -   Having low residuals is not enough"
      ],
      "id": "ea14410e-e887-44b6-8458-37aa75f14022"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta_hat_col = [0.99857143 1.99714286]\n",
            "rank=1, cols=2, norm(X*beta_hat_col-y)=0.0"
          ]
        }
      ],
      "source": [
        "y = np.array([5.0, 10.1, 14.9])\n",
        "beta_hat, residuals, rank, s = scipy.linalg.lstsq(X_col, y)\n",
        "print(f\"beta_hat_col = {beta_hat}\")\n",
        "print(f\"rank={rank}, cols={X.shape[1]}, norm(X*beta_hat_col-y)={norm(residuals)}\")"
      ],
      "id": "31309c90"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fat Design Matrices\n",
        "\n",
        "-   Fat $\\mathbb{R}^{N\\times M}$ “design matrices” have $N < M$ and are\n",
        "    “underdetermined”\n",
        "-   Less common in econometrics, but useful to understand the structure\n",
        "-   A continuum $\\beta\\in\\mathbb{R}^{M - \\text{rank}(X)}$ solve this\n",
        "    problem"
      ],
      "id": "a0a8b58c-a6b6-4f08-92ad-d759ff057d2e"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta_hat = [0.8 0.6 1. ], rank=2, ? residuals = []"
          ]
        }
      ],
      "source": [
        "X = np.array([[1, 2, 3], [0, 5, 7]]) # 2 rows, 3 variables\n",
        "y = np.array([5, 10])\n",
        "beta_hat, residuals, rank, s = scipy.linalg.lstsq(X, y)\n",
        "print(f\"beta_hat = {beta_hat}, rank={rank}, ? residuals = {residuals}\")"
      ],
      "id": "c526e394"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Which Solution?\n",
        "\n",
        "-   Residuals are zero here because there are enough parameters to fit\n",
        "    perfectly (i.e., it is underdetermined)\n",
        "-   Given the multiple solutions, the `lstsq` is giving\n",
        "\n",
        "$$\n",
        "\\min_{\\beta} ||\\beta||_2^2 \\text{ s.t. } X \\beta = y\n",
        "$$\n",
        "\n",
        "-   i.e., the “smallest” coefficients which interpolate the data exactly\n",
        "-   Which trivially fulfills the OLS objective:\n",
        "    $\\min_{\\beta} ||y - X \\beta||_2^2$\n",
        "\n",
        "## Careful Interpreting Underdetermined Solutions\n",
        "\n",
        "-   Useful and common in ML, but be **very** careful when interpreting\n",
        "    for economics\n",
        "    -   Tight connections to Bayesian versions of statistical tests\n",
        "    -   But until you understand econometrics and “identification” well,\n",
        "        **stick to full-rank matrices**\n",
        "    -   **Advanced topics:** search for “Regularization”, “Ridgeless\n",
        "        Regression” and “Benign Overfitting in Linear Regression.”"
      ],
      "id": "bbc2114d-f640-4644-a216-6bab012170e0"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  }
}